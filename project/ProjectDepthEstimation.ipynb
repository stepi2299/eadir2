{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VFI1yr8ckfgi8usxVPPNGzHMncDIqgPp","timestamp":1684259536742},{"file_id":"1509_kRE3kr7WLr8mIrQyjTaDg0JfJUlC","timestamp":1680181390788}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Depth Estimation"],"metadata":{"id":"dVXejscrTgwc"}},{"cell_type":"markdown","source":["## Connect with Google Drive"],"metadata":{"id":"Ix6C9ywdTX64"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount=True)"],"metadata":{"id":"eDBKqqPPo5wV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Parameters"],"metadata":{"id":"XblUTSVNUnf2"}},{"cell_type":"code","source":["LEARNING_RATE = 1e-3\n","BATCH_SIZE = 16\n","ANNOTATIONS_FILE_PATH = '/content/gdrive/MyDrive/data/nyu_samples/nyu2_test.csv'\n","IMAGE_FOLDER_PATH = '/content/gdrive/MyDrive/data/nyu_samples/nyu2_test'\n","DATA_AUGMENTATION = True\n","MAX_EPOCHS = 10"],"metadata":{"id":"rR8NjR_pUn2v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","## Dependencies\n"],"metadata":{"id":"MQPtpDerTWwD"}},{"cell_type":"code","source":["!pip install pytorch_lightning"],"metadata":{"id":"2WZmk3HNUgqs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import math\n","import pandas as pd\n","import cv2 as cv\n","import copy as cp\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","from torchvision.models import efficientnet_b0\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning import loggers as pl_loggers"],"metadata":{"id":"2Y6yObFeTng9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating Dataset"],"metadata":{"id":"Ej-PoHA8Umt_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lBuOjehhXuI"},"outputs":[],"source":["class NYUDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = Image.open(img_path).convert('RGB')\n","        mask_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n","        mask = Image.open(mask_path)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            mask = self.target_transform(mask)\n","        if isinstance(mask, torch.Tensor):\n","            mask = mask.to(torch.float32)\n","        else:\n","            mask = mask.asarray(\"float32\")\n","        return image, mask\n"]},{"cell_type":"code","source":["def visualize_depth_map(data, index=None):\n","  if index:\n","    imgs = data[index]\n","    img, depth = visualize_img(imgs)\n","    figure, axs = plt.subplots(1, 2, figsize=(10,5))\n","    figure.suptitle(f'Image with depth map, index {index}')\n","    axs[0].set_title(\"Base image\")\n","    axs[0].imshow(img)\n","    axs[1].set_title(\"Depth Map\")\n","    axs[1].imshow(depth)\n","  else:\n","    print(\"Show 20 first image pairs\")\n","    figure, axs = plt.subplots(20, 2, figsize=(10,20*5))\n","    for i, imgs in enumerate(data):\n","      if i == 20:\n","        break\n","      img, depth = visualize_img(imgs)\n","      axs[i, 0].set_title(f\"Base image with index: {i}\")\n","      axs[i, 0].imshow(img)\n","      axs[i, 1].set_title(\"Depth Map\")\n","      axs[i, 1].imshow(depth)\n","      \n","def visualize_img(imgs):\n","  imgs = cp.deepcopy(imgs)\n","  img, depth = imgs\n","  img = img.detach().cpu().numpy()\n","  r = img[0]\n","  g = img[1]\n","  b = img[2]\n","  img = (cv.merge([r, g, b])+1)*125\n","  img = img.astype(\"uint8\")\n","  return img, depth"],"metadata":{"id":"S1B3eCkee9e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if DATA_AUGMENTATION:\n","  trans = transforms.Compose(\n","      [\n","          transforms.ToTensor(),\n","          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","          transforms.Resize((224, 224))\n","      ])\n","else:\n","  trans = transforms.Compose(\n","      [\n","          transforms.ToTensor(),\n","      ])\n","def visual_mask_fn(x):\n","  x = np.array(x) / 10000\n","  x = cv.resize(x, (56, 56))\n","  return torch.tensor(x)\n","  "],"metadata":{"id":"lo5Gnxf_ykyC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BiFPN (Decoder for task depth estimation)"],"metadata":{"id":"xSH5evJdHYkL"}},{"cell_type":"code","source":["class BiFPN(nn.Module):\n","    def __init__(self,  fpn_sizes):\n","        super(BiFPN, self).__init__()\n","        \n","        P4_channels, P5_channels, P6_channels = fpn_sizes\n","        self.W_bifpn = 64\n","\n","        #self.p6_td_conv  = nn.Conv2d(P6_channels, self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p6_td_conv  = nn.Conv2d(P6_channels, self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p6_td_conv_2  = nn.Conv2d(self.W_bifpn, self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p6_td_act   = nn.ReLU()\n","        self.p6_td_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p6_td_w1    = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","\n","        self.p5_td_conv  = nn.Conv2d(P5_channels,self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p5_td_conv_2  = nn.Conv2d(self.W_bifpn,self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p5_td_act   = nn.ReLU()\n","        self.p5_td_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p5_td_w1    = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p5_td_w2    = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p6_upsample  = nn.Upsample(scale_factor=2, mode='nearest')\n","\n","        self.p4_td_conv  = nn.Conv2d(P4_channels, self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p4_td_conv_2  = nn.Conv2d(self.W_bifpn, self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p4_td_act   = nn.ReLU()\n","        self.p4_td_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p4_td_w1    = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p4_td_w2    = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p5_upsample   = nn.Upsample(scale_factor=2, mode='nearest')\n","\n","\n","\n","        #self.p4_out_conv = nn.Conv2d(P4_channels, self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p4_out_conv = nn.Conv2d(self.W_bifpn, self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p4_out_act   = nn.ReLU()\n","        self.p4_out_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p4_out_w1   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p4_out_w2   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","\n","        #self.p5_out_conv = nn.Conv2d(P5_channels,self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p5_out_conv = nn.Conv2d(self.W_bifpn,self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p5_out_act   = nn.ReLU()\n","        self.p5_out_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p5_out_w1   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p5_out_w2   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p5_out_w3   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p4_downsample= nn.MaxPool2d(kernel_size=2)\n","\n","        #self.p6_out_conv = nn.Conv2d(P6_channels, self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p6_out_conv = nn.Conv2d(self.W_bifpn, self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p6_out_act   = nn.ReLU()\n","        self.p6_out_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p6_out_w1   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p6_out_w2   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p6_out_w3   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p5_downsample= nn.MaxPool2d(kernel_size=2)\n","\n","\n","\n","    def forward(self, inputs):\n","        epsilon = 0.0001\n","        P4, P5, P6 = inputs\n","\n","        P6_td_inp = self.p6_td_conv(P6)\n","        P6_td = self.p6_td_conv_2((self.p6_td_w1 * P6_td_inp) / (self.p6_td_w1 + epsilon))\n","        P6_td = self.p6_td_act(P6_td)\n","        P6_td = self.p6_td_conv_bn(P6_td)\n","\n","         \n","        P5_td_inp = self.p5_td_conv(P5)\n","        P5_td = self.p5_td_conv_2((self.p5_td_w1 * P5_td_inp + self.p5_td_w2 * self.p6_upsample(P6_td)) /\n","                                 (self.p5_td_w1 + self.p5_td_w2 + epsilon))\n","        P5_td = self.p5_td_act(P5_td)\n","        P5_td = self.p5_td_conv_bn(P5_td)\n","\n","        P4_td_inp = self.p4_td_conv(P4)\n","        P4_td = self.p4_td_conv_2((self.p4_td_w1 * P4_td_inp + self.p4_td_w2 * self.p5_upsample(P5_td)) /\n","                                 (self.p4_td_w1 + self.p4_td_w2 + epsilon))\n","        P4_td = self.p4_td_act(P4_td)\n","        P4_td = self.p4_td_conv_bn(P4_td)\n","\n","\n","\n","        P4_out = self.p4_out_conv((self.p4_out_w1 * P4_td_inp  + self.p4_out_w2 * P4_td)\n","                                    / (self.p4_out_w1 + self.p4_out_w2 + epsilon))\n","        P4_out = self.p4_out_act(P4_out)\n","        P4_out = self.p4_out_conv_bn(P4_out)\n","\n","        \n","        P5_out = self.p5_out_conv(( self.p5_out_w1 * P5_td_inp + self.p5_out_w2 * P5_td + self.p5_out_w3 * self.p4_downsample(P4_out) )\n","                                    / (self.p5_out_w2 + self.p5_out_w3 + epsilon))\n","        P5_out = self.p5_out_act(P5_out)\n","        P5_out = self.p5_out_conv_bn(P5_out)\n","\n","        \n","        P6_out = self.p6_out_conv((self.p6_out_w1 * P6_td_inp + self.p6_out_w2 * P6_td + self.p6_out_w3 * self.p5_downsample(P5_out) )\n","                                    / (self.p6_out_w1 + self.p6_out_w2 + self.p6_out_w3 + epsilon))\n","        P6_out = self.p6_out_act(P6_out)\n","        P6_out = self.p6_out_conv_bn(P6_out)\n","\n","        \n","\n","        return [P4_out, P5_out, P6_out]"],"metadata":{"id":"1CB8F0PiizQE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EfficientNet (encoder for task depth estimation)\n","\n"],"metadata":{"id":"VDxu7QTcHq6e"}},{"cell_type":"code","source":["class EfficientNet(nn.Module):\n","  def __init__(self):\n","    super(EfficientNet, self).__init__()\n","    encoder = efficientnet_b0(pretrained=True)\n","    features = encoder.features\n","    num_of_sequences = len(features)\n","    self.layer1 = features[:num_of_sequences-6]\n","    self.layer2 = features[num_of_sequences-6]\n","    self.layer3 = features[num_of_sequences-5]\n","\n","  def get_features(self, x):\n","    x1 = self.layer1(x)\n","    x2 = self.layer2(x1)\n","    x3 = self.layer3(x2)\n","    return x1,x2,x3"],"metadata":{"id":"2BuAnrtm3Ic3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Custiom Model (Based on EfficientDet)"],"metadata":{"id":"cj4I1Y3pW-WC"}},{"cell_type":"code","source":["class CustomEfficientDet(nn.Module):\n","  def __init__(self):\n","    super(CustomEfficientDet, self).__init__()\n","    self.encoder = EfficientNet()\n","    self.decoder_1 = BiFPN([24, 40, 80])\n","    self.decoder_2 = BiFPN([64, 64, 64])\n","    self.decoder_3 = BiFPN([64, 64, 64])\n","    self.upsample_4 = nn.Upsample(scale_factor=4, mode='nearest')\n","    self.upsample_2   = nn.Upsample(scale_factor=2, mode='nearest')\n","    self.final_convolution = nn.Conv2d(in_channels=64*3, out_channels=1, kernel_size=3, padding=\"same\")\n","\n","  def forward(self, x):\n","    p4, p5, p6 = self.encoder.get_features(x)\n","    out_dec_1 = self.decoder_1([p4, p5, p6])\n","    out_dec_2 = self.decoder_2(out_dec_1)\n","    out_dec_3 = self.decoder_3(out_dec_2)\n","    cat_out = self.concatenate_bifpn_features(out_dec_3)\n","    return self.final_convolution(cat_out)\n","\n","  def concatenate_bifpn_features(self, out_bfpn3):\n","    p4 = out_bfpn3[0]\n","    p5 = out_bfpn3[1]\n","    p6 = out_bfpn3[2]\n","    p6_up = self.upsample_4(p6)\n","    p5_up = self.upsample_2(p5)\n","    return torch.cat([p4, p5_up, p6_up], dim=1)"],"metadata":{"id":"vQgt3UUV5178"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create interface for training using lightning module"],"metadata":{"id":"n7ryobJ0XNgL"}},{"cell_type":"code","source":["class LightningEfficientDet(pl.LightningModule):\n","  def __init__(self, batch_size=4, learning_rate=1e-3):\n","    super(LightningEfficientDet, self).__init__()\n","    self.batch_size = batch_size\n","    self.efficient_det = CustomEfficientDet()\n","    self.loss = nn.MSELoss()\n","    self.validation_step_losses = []\n","    self.learning_rate = learning_rate\n","\n","  def forward(self, x):\n","    return self.efficient_det(x)\n","\n","  def training_step(self, train_batch, batch_idx):\n","    x, y = train_batch\n","    batch_out = self.forward(x)\n","    loss = self.loss(batch_out, y)\n","    logs = {'train_loss': loss}\n","    return {'loss': loss, 'log': logs}\n","\n","  def validation_step(self, val_batch, batch_idx):\n","    x, y = val_batch\n","    batch_out = self.forward(x)\n","    loss = self.loss(batch_out, y)\n","    logs = {'train_loss': loss}\n","    self.validation_step_losses.append(loss)\n","    return {'batch_val_loss': loss}\n","\n","  def on_validation_epoch_end(self) -> None:\n","    avg_loss = torch.stack(self.validation_step_losses).mean()\n","    tensorboard_logs = {'val_loss': avg_loss}\n","    self.log(\"val_loss\", avg_loss)\n","\n","  def prepare_data(self):\n","    self.train_dataset = NYUDataset(annotations_file=ANNOTATIONS_FILE_PATH, img_dir=IMAGE_FOLDER_PATH, transform=trans, target_transform=visual_mask_fn)\n","    self.val_dataset = NYUDataset(annotations_file=ANNOTATIONS_FILE_PATH, img_dir=IMAGE_FOLDER_PATH, transform=trans, target_transform=visual_mask_fn)  # split datasets\n","    \n","  def train_dataloader(self):\n","     return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=2,shuffle=True)\n","\n","  def val_dataloader(self):\n","     return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=2,shuffle=True)\n","\n","  def configure_optimizers(self):\n","     optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5)\n","     return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}"],"metadata":{"id":"ui7hUsOKmlM3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"5zbAXjbyVjp5"}},{"cell_type":"code","source":["early_stop_callback = pytorch_lightning.callbacks.EarlyStopping(\n","monitor='val_loss',\n","min_delta=0.0,\n","patience=10,\n","verbose=False,\n","mode='min'\n",")\n","\n","\n","checkpoint_callback = ModelCheckpoint(\n","    filename='train',\n","    save_top_k=1,\n","    verbose=True,\n","    monitor='val_loss',\n","    mode='min',\n",")\n","\n","tb_logger = pl_loggers.TensorBoardLogger('/content/gdrive/MyDrive/data/record', name='project')\n","model = LightningEfficientDet(batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE)\n","trainer = pl.Trainer(\n","    callbacks=[early_stop_callback, checkpoint_callback],  max_epochs=MAX_EPOCHS, \n","  logger=tb_logger)"],"metadata":{"id":"XGyg0TEiuYuW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.fit(model)"],"metadata":{"id":"wa7CCb-6ugsS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tests"],"metadata":{"id":"jlotDGA2Sz9i"}},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir '/content/gdrive/MyDrive/data/record'"],"metadata":{"id":"xxN5Lb98Xl1P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = NYUDataset(annotations_file=ANNOTATIONS_FILE_PATH, img_dir=IMAGE_FOLDER_PATH, transform=trans, target_transform=visual_mask_fn)\n","dataloader =  DataLoader(data, batch_size=BATCH_SIZE, num_workers=2,shuffle=True)"],"metadata":{"id":"0TZfsvXBZzZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_test, mask = next(iter(dataloader))\n","out = model(img_test)[0][0]\n","out_img = out.detach().cpu().numpy()*1000"],"metadata":{"id":"zeGncIhwvIhw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["figure, axs = plt.subplots(1, 2, figsize=(10,5))\n","figure.suptitle(f'Image with depth map')\n","axs[0].set_title(\"Base image\")\n","axs[0].imshow(img_test[0][0])\n","axs[1].set_title(\"Mask\")\n","axs[1].imshow(out_img, cmap=\"gray\")"],"metadata":{"id":"R8CitepbRahA"},"execution_count":null,"outputs":[]}]}