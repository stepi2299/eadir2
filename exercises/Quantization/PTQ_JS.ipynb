{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#TASK:\n",
    "1. Split the 1000 samples into two groups, 50%,50%, one of them will be named calibration_dataset while the other will be validation_dataset. You should use calibration dataset to calibrate scale and zero point before performing actual quantization.\n",
    "2. Make PTQ training for **torchvision.models.quantization.mobilenet_v2**, using PerchannelMinMax Quantization for weights and PerTensorMinMax Quantization separately.\n",
    "3. Make Perchannel MovingAverageMinMax Quantization for weights and MovingAverage Pertensor Quantization for activation. Compare to results in 2., Check which one is better. Compare some scales and zero points of from the observers. Explain why one of the solution is better?"
   ],
   "metadata": {
    "id": "pICFkTj7km9n"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJ9Tbd62GBjQ"
   },
   "outputs": [],
   "source": [
    "!gdown 1oHoYT7J4-xKfNu6cfBOMKHyO0QBqdYsI\n",
    "!unzip /content/calibration_data2.zip -d /content/calibration_data"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.quantization.qconfig import QConfig\n",
    "from torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, MovingAveragePerChannelMinMaxObserver, PerChannelMinMaxObserver\n",
    "from torch.ao.quantization.fake_quantize import FakeQuantize\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "import copy\n",
    "from torch.ao.quantization import (\n",
    "  get_default_qconfig_mapping,\n",
    "  get_default_qat_qconfig_mapping,\n",
    "  QConfigMapping,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "import time"
   ],
   "metadata": {
    "id": "C6s0M5cqGFOU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "transform_cali = transforms.Compose([\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "      transforms.Normalize(\n",
    "          mean=[0.485, 0.456, 0.406],  # ImageNet dataset mean\n",
    "          std=[0.229, 0.224, 0.225]  # ImageNet dataset standard deviation\n",
    "      )\n",
    "  ])\n",
    "transform_val = transforms.Compose([\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "      transforms.Normalize(\n",
    "          mean=[0.485, 0.456, 0.406],  # ImageNet dataset mean\n",
    "          std=[0.229, 0.224, 0.225]  # ImageNet dataset standard deviation\n",
    "      )\n",
    "  ])\n"
   ],
   "metadata": {
    "id": "JApUvwubGy8y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import os\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_file, calibration=True, transform=None, dir=None):\n",
    "        self.data = []\n",
    "        with open(txt_file, 'r') as f:\n",
    "            for line in f:\n",
    "                image_path = line.split(' ')[0]\n",
    "                label = line.split(' ')[1].split('\\n')[0]\n",
    "                self.data.append((image_path, int(label)))\n",
    "            nr_images = len(self.data)\n",
    "            if calibration:\n",
    "                self.data = self.data[:nr_images//2]\n",
    "            else:\n",
    "                self.data = self.data[nr_images//2:]\n",
    "        self.transform = transform\n",
    "        self.dir = dir\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        if self.dir:\n",
    "            image_path = os.path.join(self.dir, image_path)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, original_dataset, samples_per_epoch):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.samples_per_epoch = samples_per_epoch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        random_index = random.randint(0, len(self.original_dataset) - 1)\n",
    "        return self.original_dataset[random_index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples_per_epoch"
   ],
   "metadata": {
    "id": "V96u5oWgL5SP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "calibration_dataset = MyDataset('/content/calibration_data/samples.txt', calibration=True, transform=transform_cali)\n",
    "val_dataset = MyDataset('/content/calibration_data/samples.txt', calibration=False, transform=transform_val)"
   ],
   "metadata": {
    "id": "Lg8pVJv8L9wa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define Model which will be quantized"
   ],
   "metadata": {
    "id": "N2Bib2jzVcMf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "example_inputs = (torch.randn(1, 3, 224, 224),)\n",
    "model_fp = torchvision.models.quantization.mobilenet_v2(pretrained=True)\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "quantize_fx.fuse_fx(model_to_quantize.eval())"
   ],
   "metadata": {
    "id": "HvGXicR7VcVw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Specify quantization configuration"
   ],
   "metadata": {
    "id": "Ke6VOVtATzrY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "qconfig_1 = QConfig(\n",
    "    activation=FakeQuantize.with_args(\n",
    "        observer=MinMaxObserver,\n",
    "        quant_min=0,\n",
    "        quant_max=255,\n",
    "        qscheme=torch.per_tensor_affine,\n",
    "        reduce_range=False,),\n",
    "    weight=FakeQuantize.with_args(\n",
    "        observer=PerChannelMinMaxObserver,\n",
    "        quant_min=-128,\n",
    "        quant_max=127,\n",
    "        dtype=torch.qint8,\n",
    "        qscheme=torch.per_channel_symmetric,\n",
    "        reduce_range=False,\n",
    "        ch_axis=0,\n",
    "          ))"
   ],
   "metadata": {
    "id": "wmm5k0PSIeDX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "qconfig_2 = QConfig(\n",
    "    activation=FakeQuantize.with_args(\n",
    "        observer=MinMaxObserver,\n",
    "        quant_min=0,\n",
    "        quant_max=255,\n",
    "        qscheme=torch.per_tensor_affine,\n",
    "        reduce_range=False,),\n",
    "    weight=FakeQuantize.with_args(\n",
    "        observer=MinMaxObserver,\n",
    "        quant_min=-128,\n",
    "        quant_max=127,\n",
    "        dtype=torch.qint8,\n",
    "        qscheme=torch.per_tensor_symmetric,\n",
    "        reduce_range=False\n",
    "        ))"
   ],
   "metadata": {
    "id": "HtX2PrjIg1uB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "qconfig_3 = QConfig(\n",
    "    activation=FakeQuantize.with_args(\n",
    "        observer=MovingAverageMinMaxObserver,\n",
    "        quant_min=0,\n",
    "        quant_max=255,\n",
    "        qscheme=torch.per_tensor_affine,\n",
    "        reduce_range=False,),\n",
    "    weight=FakeQuantize.with_args(\n",
    "        observer=MovingAveragePerChannelMinMaxObserver,\n",
    "        quant_min=-128,\n",
    "        quant_max=127,\n",
    "        dtype=torch.qint8,\n",
    "        qscheme=torch.per_channel_symmetric,\n",
    "        reduce_range=False,\n",
    "        ch_axis=0\n",
    "        ))"
   ],
   "metadata": {
    "id": "0XV5zSBkiEOX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "qconfig_mapping = QConfigMapping().set_global(qconfig_3)\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)"
   ],
   "metadata": {
    "id": "kMndu6hOdIdr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing model evaluation"
   ],
   "metadata": {
    "id": "qhafzZiiUSuR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader, desc=\"Batches\", position=0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    inference_time = time.perf_counter() - start_time\n",
    "\n",
    "    return avg_loss, accuracy, inference_time"
   ],
   "metadata": {
    "id": "CnvDYm45M0xP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Create training/inference/evaluation tools\n"
   ],
   "metadata": {
    "id": "V0mQtZ7-WYiB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "validation_loader = DataLoader(val_dataset, batch_size=32, shuffle=False,drop_last=True)\n",
    "calibration_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "id": "BVhlXRWhNM7O"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of base model"
   ],
   "metadata": {
    "id": "YUoLMikHWbME"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "loss, acc, perf_time = evaluate_model(model_prepared, validation_loader, criterion, \"cpu\")"
   ],
   "metadata": {
    "id": "01znXw1-Qprr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Accuracy\", acc)\n",
    "print(\"Performing Time\", perf_time)\n",
    "print(\"Loss\", loss)"
   ],
   "metadata": {
    "id": "_qLIhKywRr-L"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calibration of the Quantization"
   ],
   "metadata": {
    "id": "kXy0HhB4YN5n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_prepared = model_prepared.to('cpu')\n",
    "for data,labels in tqdm(calibration_loader, desc=\"Batches\", position=0):\n",
    "    model_prepared(data)"
   ],
   "metadata": {
    "id": "L63wdI2tRnIl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_quantized = quantize_fx.convert_fx(model_prepared)"
   ],
   "metadata": {
    "id": "Krn6CuqrRFDa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for module in model_quantized.modules():\n",
    "  if isinstance(module, torch.quantization.ObserverBase):\n",
    "        print(f\"For module {module}: Scale = {module.scale}, Zero Point = {module.zero_point}\")"
   ],
   "metadata": {
    "id": "M53K0YeZieH3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation of the quantized model"
   ],
   "metadata": {
    "id": "SPYhJiFTZiyw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "loss, acc, perf_time = evaluate_model(model_quantized, validation_loader, criterion, \"cpu\")"
   ],
   "metadata": {
    "id": "726p1s-2RHG1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Accuracy\", acc)\n",
    "print(\"Performing Time\", perf_time)\n",
    "print(\"Loss\", loss)"
   ],
   "metadata": {
    "id": "N8tjInGJRt_N"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_prepared"
   ],
   "metadata": {
    "id": "I0tBh2pUTJY4"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
